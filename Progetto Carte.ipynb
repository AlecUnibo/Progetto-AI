{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform=None, yaml_file=\"C:/Users/ale03/OneDrive/Desktop/Progetto di AI/archive/data.yaml\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (string): Percorso alla directory con le immagini.\n",
    "            labels_dir (string): Percorso alla directory con le etichette.\n",
    "            transform (callable, optional): Trasformazioni da applicare alle immagini.\n",
    "            yaml_file (string): Percorso al file YAML contenente le classi.\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        try:\n",
    "            with open(yaml_file, \"r\") as file:\n",
    "                data = yaml.safe_load(file)\n",
    "                self.classes = data.get(\"names\", [])  # La lista di classi del file data.yaml\n",
    "                print(f\"Classi caricate: {self.classes}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento del file YAML: {e}\")\n",
    "            self.classes = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.images_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ottieni il nome dell'immagine e il percorso\n",
    "        img_name = os.path.join(self.images_dir, os.listdir(self.images_dir)[idx])\n",
    "        label_name = os.path.join(self.labels_dir, os.listdir(self.labels_dir)[idx].replace(\".jpg\", \".txt\"))\n",
    "        \n",
    "        # Leggi le etichette dal file .txt\n",
    "        with open(label_name, 'r') as file:\n",
    "            label_info = file.readline().split()\n",
    "        \n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = int(label_info[0])  # La prima colonna Ã¨ l'etichetta numerica\n",
    "        xmin, ymin, xmax, ymax = map(float, label_info[1:])\n",
    "\n",
    "        # Normalizza il bounding box\n",
    "        image_width, image_height = image.size\n",
    "        bbox = torch.tensor([xmin / image_width, ymin / image_height, xmax / image_width, ymax / image_height], dtype=torch.float)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, bbox, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Filtra gli elementi None\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:  # Nessun elemento valido\n",
    "        return None\n",
    "    images, labels = zip(*batch)\n",
    "    return torch.stack(images, 0), torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definizione del dataset personalizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 128)  # Adatta al tuo input\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN(num_classes=52)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trasformazioni per le immagini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset e DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classi caricate: ['10c', '10d', '10h', '10s', '2c', '2d', '2h', '2s', '3c', '3d', '3h', '3s', '4c', '4d', '4h', '4s', '5c', '5d', '5h', '5s', '6c', '6d', '6h', '6s', '7c', '7d', '7h', '7s', '8c', '8d', '8h', '8s', '9c', '9d', '9h', '9s', 'Ac', 'Ad', 'Ah', 'As', 'Jc', 'Jd', 'Jh', 'Js', 'Kc', 'Kd', 'Kh', 'Ks', 'Qc', 'Qd', 'Qh', 'Qs']\n",
      "Batch di immagini: torch.Size([32, 3, 224, 224])\n",
      "Bounding box: tensor([[4.7095e-04, 9.3322e-04, 1.5891e-04, 7.8009e-05],\n",
      "        [1.2453e-03, 1.1933e-03, 1.6758e-04, 7.2231e-05],\n",
      "        [2.0109e-03, 8.7833e-04, 2.3981e-04, 1.0690e-04],\n",
      "        [6.9631e-04, 1.1095e-03, 2.4270e-04, 1.0112e-04],\n",
      "        [5.9229e-04, 8.1765e-04, 1.0401e-04, 1.5602e-04],\n",
      "        [1.2048e-03, 7.0208e-04, 8.0899e-05, 1.6469e-04],\n",
      "        [5.9229e-04, 7.7432e-04, 1.9936e-04, 1.8491e-04],\n",
      "        [9.3900e-04, 5.9229e-04, 2.3114e-04, 1.2424e-04],\n",
      "        [6.7897e-04, 1.7653e-03, 2.0803e-04, 1.9069e-04],\n",
      "        [1.9791e-03, 8.0032e-04, 2.3981e-04, 9.8234e-05],\n",
      "        [1.1239e-03, 1.9040e-03, 8.9566e-05, 1.5891e-04],\n",
      "        [1.3984e-03, 1.6873e-03, 9.8234e-05, 1.5602e-04],\n",
      "        [1.0141e-03, 1.5255e-03, 2.3692e-04, 1.2713e-04],\n",
      "        [7.4831e-04, 6.9631e-04, 1.3002e-04, 1.4446e-04],\n",
      "        [1.9936e-03, 7.1075e-04, 1.3002e-04, 1.4157e-04],\n",
      "        [1.8087e-03, 1.5977e-03, 1.4157e-04, 2.3114e-04],\n",
      "        [4.8828e-04, 1.0026e-03, 1.6469e-04, 1.0979e-04],\n",
      "        [4.4783e-04, 5.2873e-04, 9.2456e-05, 1.5891e-04],\n",
      "        [1.8953e-03, 1.0719e-03, 9.2456e-05, 1.5891e-04],\n",
      "        [9.9679e-04, 6.9053e-04, 1.8491e-04, 2.2825e-04],\n",
      "        [1.1961e-03, 1.2770e-03, 1.5024e-04, 1.3579e-04],\n",
      "        [1.9011e-03, 9.9968e-04, 2.3403e-04, 1.1268e-04],\n",
      "        [1.8202e-03, 1.4302e-03, 1.5891e-04, 6.9342e-05],\n",
      "        [1.2915e-03, 3.6404e-04, 2.3403e-04, 1.7335e-04],\n",
      "        [1.0864e-03, 3.5249e-04, 1.5602e-04, 2.2825e-04],\n",
      "        [1.3897e-03, 8.0032e-04, 1.5891e-04, 1.0690e-04],\n",
      "        [1.1990e-03, 1.4966e-03, 1.9069e-04, 1.7913e-04],\n",
      "        [5.9807e-04, 7.8876e-04, 1.0979e-04, 1.5891e-04],\n",
      "        [1.5169e-03, 1.6555e-03, 1.9647e-04, 1.9647e-04],\n",
      "        [1.3608e-03, 1.2193e-03, 1.5891e-04, 8.0899e-05],\n",
      "        [1.4446e-03, 1.1528e-03, 1.6469e-04, 7.2231e-05],\n",
      "        [1.4706e-03, 1.2395e-03, 1.0401e-04, 1.5024e-04]])\n",
      "Labels: tensor([22, 21, 28,  9,  4, 24, 18, 36,  2, 51, 26, 18, 41,  2, 51, 20, 45, 49,\n",
      "        12,  0,  0, 48, 34, 19, 35, 29, 37,  5, 48, 38, 19, 12])\n"
     ]
    }
   ],
   "source": [
    "# Percorsi per le immagini e le etichette\n",
    "train_images_dir = \"C:/Users/ale03/OneDrive/Desktop/Progetto di AI/archive/train/images\"\n",
    "train_labels_dir = \"C:/Users/ale03/OneDrive/Desktop/Progetto di AI/archive/train/labels\"\n",
    "\n",
    "# Trasformazioni per il dataset\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Creazione del dataset e del dataloader\n",
    "train_dataset = CardDataset(images_dir=train_images_dir, labels_dir=train_labels_dir, transform=train_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Test del DataLoader\n",
    "for images, bboxes, labels in train_loader:\n",
    "    print(f\"Batch di immagini: {images.size()}\")\n",
    "    print(f\"Bounding box: {bboxes}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modello AlexNet pre-addestrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ale03\\miniconda3\\envs\\corso_ai\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ale03\\miniconda3\\envs\\corso_ai\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 52)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sposta il modello sulla GPU se disponibile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzione di perdita e ottimizzatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciclo di addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 1, Loss: 3.9311, Train Accuracy: 3.78%\n",
      "Epoca 2, Loss: 3.0591, Train Accuracy: 28.01%\n",
      "Epoca 3, Loss: 2.1557, Train Accuracy: 62.34%\n",
      "Epoca 4, Loss: 1.2275, Train Accuracy: 89.47%\n",
      "Epoca 5, Loss: 0.5203, Train Accuracy: 99.03%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, bboxes, labels in train_loader:  # Adatta al dataset con 3 valori\n",
    "        label_indices = labels.to(device)  # Le etichette sono giÃ  numeriche\n",
    "        \n",
    "        images = images.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, label_indices)  # Usa le etichette numeriche come target\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == label_indices).sum().item()\n",
    "        total += label_indices.size(0)\n",
    "\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoca {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione sul test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classi caricate: ['10c', '10d', '10h', '10s', '2c', '2d', '2h', '2s', '3c', '3d', '3h', '3s', '4c', '4d', '4h', '4s', '5c', '5d', '5h', '5s', '6c', '6d', '6h', '6s', '7c', '7d', '7h', '7s', '8c', '8d', '8h', '8s', '9c', '9d', '9h', '9s', 'Ac', 'Ad', 'Ah', 'As', 'Jc', 'Jd', 'Jh', 'Js', 'Kc', 'Kd', 'Kh', 'Ks', 'Qc', 'Qd', 'Qh', 'Qs']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 10.68%\n"
     ]
    }
   ],
   "source": [
    "# Caricamento del dataset di test\n",
    "test_images_dir = \"C:/Users/ale03/OneDrive/Desktop/Progetto di AI/archive/test/images\"\n",
    "test_labels_dir = \"C:/Users/ale03/OneDrive/Desktop/Progetto di AI/archive/test/labels\"\n",
    "\n",
    "test_dataset = CardDataset(images_dir=test_images_dir, labels_dir=test_labels_dir, transform=train_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Valutazione del modello\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, bboxes, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizzazione delle predizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(dataloader, model, class_names, num_images=5):\n",
    "    model.eval()\n",
    "    images_shown = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                if images_shown >= num_images:\n",
    "                    return\n",
    "\n",
    "                img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "                img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]  # De-normalizza\n",
    "                img = np.clip(img, 0, 1)\n",
    "\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Predetto: {class_names[preds[i]]}, Reale: {class_names[labels[i]]}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                images_shown += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mostra alcune predizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def show_predictions(dataloader, model, class_names, output_dir=\"archive\", num_images=10):\n",
    "    model.eval()\n",
    "    images_shown = 0\n",
    "\n",
    "    # Creare la cartella per salvare le immagini, se non esiste\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                if images_shown >= num_images:\n",
    "                    return\n",
    "\n",
    "                # Processa l'immagine\n",
    "                img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "                img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]  # Denormalizza\n",
    "                img = np.clip(img, 0, 1)\n",
    "\n",
    "                # Nome file per salvare l'immagine\n",
    "                filename = os.path.join(\n",
    "                    output_dir,\n",
    "                    f\"predicted_{class_names[preds[i]]}_actual_{class_names[labels[i]]}_{images_shown}.png\"\n",
    "                )\n",
    "\n",
    "                # Salva l'immagine\n",
    "                plt.imsave(filename, img)\n",
    "                print(f\"Immagine salvata: {filename}\")\n",
    "\n",
    "                images_shown += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corso_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
